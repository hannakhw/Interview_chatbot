# AI 부트캠프를 통해 배운 것들

1. 커뮤니티, 잘 만든 코드는 보고 배우자 
개인적인 일화인데요. 저는 (특정 분야) 플랫폼이 되겠다는 야심찬 목표를 세운 스타트업에서 일을 했던 적이 있습니다. 이용자들에게 다양한 경험을 시켜줄 수 있도록 다양한 보상 방법을 구상하기도 했고, 여러 실험들을 하기도 했는데요. 캐글에 대해 설명을 들어보니, 커뮤니티라는 특성을 가졌기 때문에 잘 된 대표적인 사례처럼 느껴졌습니다. (커뮤니티를 조성한다는 것이 생각보다 얼마나 어려운 일인지를 몸소 깨달았기 때문일까요) 어쨌든, 커뮤니티라는 기능은 플랫폼에서 빠질 수 없는 기능이 된 것 같습니다. 
캐글은 기업이 "문제"를 올리면 데이터 사이언티스트들이 문제를 "해결"하는 방식으로, 둘을 연결하는 플랫폼으로 서비스가 시작됐습니다. 물론 지금도 기본적인 형태는 "대회"가 열리면 사람들이 이를 풀어내고, 가장 좋은 성능을 만들어내는 사람이 우승하는(?) 시스템입니다. 이렇게 문제를 해결하는 과정에서 생기는 이야기들을 나눌 수 있는 '토론 공간'이 있고, 자신의 코드는 다른 사람과 공유할 수 있습니다. 반대로 말하면, 잘 모르는 사람도(뉴비들도) 전문가들이 만든 코드와 설명을 보면서 공부할 수 있다는 것이죠. 
송원호 강사님께서도 대회에 있는 Discussion과 code를 열심히 클론 코딩한 뒤에 본인의 것으로 내재화하는 방식으로 처음에는 공부해나갈 것을 추천했습니다. 예를 들어서, 내가 풀고자 하는 문제가 A라고 한다면, 캐글에 들어가 A라는 문제와 비슷한 문제를 푸는 대회(competitions)를 찾아보는 것입니다. 그리고 이곳의 주요 코드와 디스커션들을 보면서 A 문제에 적용할 수 있는 아이디어를 찾아보는 식이죠. 특히 리더보드에 있는 문제를 푼 사람들이 작성해놓은 Solution는 그냥 보물 그 자체! 입니다. (공부할 건 널렸다.) 상위 랭크된 참가자들의 솔루션을 보다 보면 요즘은 이런 기법을 쓰네? 이런 식으로도 문제를 풀 수 있네? 트렌드도 엿볼 수 있고 아이디어도 많이 얻을 수 있으니깐요. 
결국 캐글이 처음에는 문제를 올리고 사람들이 문제를 푼다는 어쩌면 단순한 형태의 플랫폼으로 시작했지만, 지금처럼 널리 퍼지게 될 수 있었던 이유는 전문가(또는 고인물?)들만 놀 수 있는 공간이 아닌, 뉴비들도 전문가들을 보며 배울 수 있고 또 성장할 수 있도록 만들어 놓은 강력한 커뮤니티 기능이 있었기 때문이 아닐까 합니다. 이 외에도 문제를 푼 사람들에게 메달을 주는 기능도 있었고, 팀을 이뤄서 협업할 수 있게 만든 기능도 있지만 가장 중요한 건, "과연 '실력자'들은 이걸 어떻게 풀었을까? 나는 이걸 보고 이런 생각을 했어!"를 얘기할 수 있는 모두를 위한 공간이였기 때문이라고 생각됩니다. 
참고로, 허깅 페이스라고 하는 플랫폼도 있는데요. 자연어처리 분야에서 NLP 모델과 도구를 개발하고 공유하는 플랫폼입니다. 캐글에 보면 '모델'이라는 란이 있는데 이 부분은 구색맞추기 같은 느낌이고 실제 모델 분야의 진짜배기 커뮤니티는 허깅페이스라고 하네요. 허깅페이스도 커뮤니티로서 왕성하게 발전해나가는 모습을 보인다고 합니다. (아직 구경안해봄)

2. 넓고 얕게보다는 좁고 깊게가 낫다
지금까지 부트캠프를 하면서 여러 번의 특강이 있었던 것으로 기억합니다. 특강들 모두 인공지능 관련 회사에서 일을 하는 현직자들의 이야기를 들을 수 있는 시간들이였는데요. 우연인지 모르겠지만, 신기하게도 모든 현직자들이 이렇게 말했습니다. "여러 개를 다양하게 접해봤고 다양한 스킬이 있다는 것보다, 단 하나라도 확실하게 파는 것이 낫다" (정확한 워딩은 아니고 이런 맥락이였습니다.)
이번 특강에서도 똑같은 얘기가 나왔습니다. 이 말이 3번쯤 반복되니 무시할 수 없습니다. 원래의 저라면 '다양한 경험이 좋아, 얕아도 괜찮아, 지루하고 안풀리면 다른 걸로 갈아타!' 이런 태도가 지배적인 인간이였는데, 부트캠프를 시작하면서부터 계속해서 이런 얘기를 들으면서 뼈를 맞으니, 이제는 끝까지 해야겠다는 생각이 듭니다... 하하 


# CV, Computer vision domain 프로젝트

이번 대회는 CV, Computer vision domain에서 중요한 이미지를 분류하는 태스크가 주제였습니다. CV의 아주 클래식한 예시처럼, 강아지 사진을 넣고 모델이 강아지인지 고양이인지 맞추게 하는 것처럼 말이지요. 대회에서 주어진 이미지들은 총 17가지 클래스로 구분되어 있었는데요. 1570장의 학습 이미지가 주어지고, 각 이미지는 클래스1(자동차 번호판), 클래스2(이력서), 클래스3(진료확인서) ... 와 같이 클래스가 라벨링된 csv 파일이 함께 제공됐습니다. 이 이미지를 학습하고 3140장의 평가 이미지를 예측하는 것이고요, 점수는 F1 score로 매겼습니다. 
가장 먼저 학습 데이터와 테스트 데이터르를 눈으로 직접 확인해봤습니다. 학습 데이터는 올바르게 정면에서 잘 찍은 듯한 이미지들이었고, 테스트 데이터는 노이즈가 심하거나 삐뚤어지고 다른 이미지와 겹쳐있는 것들도 있었습니다. 랜덤하게 회전되어 있었고, 뒤집히거나 훼손된 이미지도 있었죠. 클린한 이미지만 공부해서는 더러운 이미지까지 잘 맞추기는 어렵겠죠? 그래서 어그멘테이션이 중요할 것이라는 생각을 처음부터 했습니다. 다만, 어느 강도로 얼마나 많이, 데이터셋 크기는 어느 정도로 늘려야 할 것인가에 대해서는 감이 없었죠. 
1. 매일 팀원들과 회의
저희 팀의 경우에는 매일 아침 10시부터 10시 반까지는 각자 공부한 내용을 공유하기도 하고, 시도해 본 방법들이나 아이디어를 얘기하는 시간을 가졌습니다. 처음부터 저희 팀장을 맡아주셨던 분이 데이터 어그멘테이션을 14만장 수준으로 높이고, 베이스라인 코드에서 크게(?) 벗어나지 않은 수준에서 모델을 돌려서 결과를 제출했는데 0.9 대의 점수가 나왔습니다. 테스트용으로 올린거라 이후에 이것저것 작업하면서 코드를 잃어버리셨다고 했는데, 어쨌든 저희 팀 입장에서는 처음부터 굉장히 든든한 성과가 나와서 마음놓고 다양한 실험을 해볼 수 있는 배경이 됐어요. 
2. 데이터 전처리 파트
저는 데이터셋의 크기가 어느 정도 선에서 적당하고 효율적으로 학습시킬 수 있을 것인지 작은 실험을 만들어 검증해보았어요. (관련 포스팅) 그리고 다른 팀원께서는 학습 이미지를 일일히 눈으로 확인을 해보고 라벨링이 잘못된 이미지 8개를 발견해서 정정해주셨고요. 또 다른 팀원께서는 훈련데이터 내의 클래스별로 데이터 개수가 다르다는 걸 발견하고 임밸런스된 데이터셋보다 밸런스를 맞춘 데이터셋으로 학습을 시켰을 때 성능이 더 잘나온다는 점을 발견해주셨어요. 이미지의 평균 사이즈도 확인했는데, 사이즈가 크게 튀는 이미지가 없이 다들 비슷한 사이즈를 가지고 있다는 점도 확인해주셨어요.
3. Augmentation
첨부터 저 모든 어그멘테이션을 다 적용한 건 아니였고, 또 팀원마다 각자 넣은 방식이 조금씩 다르기도 했어요. 
저는 첨에 파이토치 라이브러리에 있던 걸 가지고 했는데 성능이 안좋아서 albumentation 라이브러리 활용해서 했고, 강도도 점차 높여갔어요. 온라인 증강에는 믹스업, 컷믹스, 컷오프 다 사용했는데 배치 돌면서 3번에 한번씩 적용되는 방식으로 했고요. 어그멘테이션도 사이클을 돌아가면서 점차 기법이 늘어나게 됐습니다. 어그멘테이션도 시행착오가 많았어요. 처음에는 자꾸 비슷한 사진들이 만들어져서 나중에는 확률값을 다르게 주면서 최대한 다양하고, 강도를 높여서 이미지를 만들려고 했어요. 대회 끝나고 이 대회를 기획한 강사님 말씀이 어그래피 라이브러리를 꼭 써볼 수 있도록 유도를 하셨다던데, 아쉽게도 저는 어그래피까지는 못써봤습니다. 
4. 모델
모델도 다양하게 써본 것 같아요. 팀원마다 써보고 싶은 것이 달라서, 본인의 관심에 맞는 것을 위주로 먼저 실험을 했고 각자 점수가 잘 나온 모델과 하이퍼파라미터를 공유했습니다. 
저는 초반에는 rasnet34, 50, densenet121, efficientb0, b4을 주로 썼고, 후반에는 다른 팀원분께서 cnn계열에서 이미 좋은 성능을 내고 계셔서 저는 swin_tiny, swin_base 모델에 집중했어요. 트랜스포머 계열의 모델로 성능을 높인 다음에 같이 하드보팅이나 소프트보팅을 해야겠다는 생각으로 그렇게 진행했고, 실제로 하드보팅을 했을 때는 결과가 꽤 잘 나왔어요. 모델을 돌리고 하이퍼파라미터를 설정하면서 정말 많은 것들을 배울 수 있었는데요. 사소한 에러로 안돌아가는 문제를 해결하는 것부터, 이미지 사이즈와 배치 사이즈의 의미, 에폭이랑 얼리 스탑은 뭘 기준으로 설정하면 좋은지, 손실함수와 옵티마이저는 무엇이 있고 서로 어떻게 다른지, 그리고 스케줄러와 같은 기능도 익히게 됐어요. 강의에서 분명 들은 적은 있었겠지만 무슨 말인지 모르고 지나갔던 것들이 이번 대회를 통해서 피부로 와닿게 느낄 수 있었던 시간들이였습니다. 
5. 컨퓨전 메트릭스
컨퓨전 메트릭스를 그려보고 정답과 예측을 비교해보는 것도 큰 도움이 됐습니다. 제가 그렸던 컨퓨전 메트릭스 그래프는 날아가버리고 없지만... 밸리데이션 셋 안에서 잘 못 맞춘 클래스들을 뽑아봤었는데, 3번, 4번, 7번, 11번, 14번을 주로 혼동하는 것을 확인했습니다. 그래서 이걸 어떻게 하면 잘 맞추게 할까... 를 고민하다가!
저희 팀원 중 한분이 3번과 7번, 4번과 14번, 3, 4, 7, 14번 클래스만 단독으로 트레인데이터셋으로 구성해 학습시킨 모델을 만들어 소프트보팅을 해보자는 의견을 제시해주셨습니다. 그렇게 한 모델을 다른 기존에 점수 높은 모델과 소프트 보팅을 하는 방식으로 앙상블하면 점수가 높아지지 않을까! 하는 생각이였죠. 그렇게 해서 나온 모델이 최종적으로는 0.9441로 가장 높은 점수를 받았습니다. (저희 팀이 올린 리더보드 상의 점수 중에서)
저는 데이터셋에서 3, 4, 7, 14만 학습시키는 방법이 아니라, 데이터셋의 구성 자체를 3, 4, 7, 14 클래스 양을 확 늘리고 다른 클래스의 이미지는 상대적으로 적은 비율을 갖도록 해서 3, 4, 7, 14를 과대학습시킨 모델을 만드는 방식을 시도했습니다. 트랜스포머 계열인 swin계열로 특정 클래스를 과대 학습시키고, 이걸 다른 cnn 계열과 앙상블하면 좋을 것 같다고 생각했어요. 사실 이때 제가 감기 몸살로 팀원들과 소통이 좀 어려웠고 아쉽게도 리더보드에는 제출 횟수 초과로 올려보지는 못했어요. 그래도 가장 높은 점수를 받은 아웃풋을 기준으로 비교해봤을 때 94%였나? 높은 수준으로 답이 유사해서 점수도 아마 비슷하지 않았을까 짐작합니다.  
6. 특히 열심히 했던 것
나는 처음에 학습데이터를 그대로 사용해서 돌려봤고, 그 다음에는 1570장의 학습데이터를 5배로 어그멘테이션시켜서 약 7800정 정도로 이것저것 실험을 했고, 어그멘테이션 기법을 늘려갈 때에도 10배, 1만 5000여장으로 학습을 시켰다. 그래서 그런지 에폭 하나 돌 때에도 2~3분 내외로 그렇게 오래 걸리지 않았는데, 다른 분들은 약 14만 장으로 오프라인 어그멘테이션으로 증강시켜서 실험을 하고, 하나 할 때마다 몇 시간씩 걸렸던 것이다. 
그래서 이런 생각이 들었다. 
학습할 데이터의 개수가 많아지면 많아질수록 모델의 학습 시간은 늘어난다. 물론 데이터 어그멘테이션을 많이 해서 다양한 데이터를 학습시키면 모델의 성능이 올라갈 것이다. 그런데 그 사이 어딘가에 최적점이 있지 않을까? 예를 들어서 데이터 이미지를 10개에서 100개 사이에서 실험을 한다고 했을 때, 10개에서 70개까지 늘릴 동안에는 성능이 쭉쭉 올라가는데, 70개에서 80개로 올렸을 때에는 시간은 더 오래걸리는데 학습 성능에 큰 차이가 없다면, 굳이 시간을 더 들여서 80개나 그 이상의 데이터로 학습할 필요가 없을 것이다.(데이터의 개수를 제외하고 다른 모든 변수는 동일하다는 전제하에) 이런 식으로, 만약 적정량의 데이터셋의 크기를 찾으면, 더 적은 데이터와 시간으로도 많은 실험을 할 수 있을 것이다. 여러 모델을 쓰고, 다른 어그멘테이션 기법을 사용해 (빠르게, 또는 효율적으로) 최적의 모델을 찾을 수 있을 것이고, 최적의 모델과 하이퍼 파라미터를 찾으면, 데이터셋을 다시 크게 늘려 모델에 학습시켜서 성능을 더 끌어올릴 수 있을 것이다.
그리고 이에 대해서 멘토님께 의견을 구했는데 이런 답변을 받았다.
대회 초반에는 토이셋을 하나 만들어서, 전체 데이터에서 밸리데이션 데이터셋과 경향이 일치하는지 확인하고, 모델도 처음부터 무거운 모델을 이용하기보다는 작고 가벼운 모델로 할 수 있는 실험들로 가설 검증을 먼저 합니다. (데이터 증강에 효과가 있는 방법은 무엇일까 와 같은 것들을 전체 데이터로 무거운 모델로 하기에는 효율이 떨어질 수 있어요.) 충분한 자원이 있다면 좋겠지만 제한된 자원과 시간이 짧을때는 위와 같이 실험하고나서 큰 모델에 적용하여 한 번 더 확인하는 방법으로 모델을 학습하기도 합니다. 다만, 이게 무조건 맞지는 않습니다. 작은 모델이나 작은 데이터셋은 각각 모델이 수렴할 수 없을만큼 어려운 문제일때가 있거나 데이터의 편중이 들어갈수도 있습니다. 그래서 가설 단계에서 토이셋으로 한 실험 중 불채택한것이 사실은 무거운 모델에서는 더 잘 될수도 있어서.. 결국에 무거운 모델과 많은 데이터에서 다시 학습을 또 해봐야 하는것 아니냐, 그러면 오히려 이전의 실험이 시간낭비 아니냐 할 수 있기도 합니다.
결국 선택의 문제!
일단 나는 성격이 급하고 빠르게 결과를 보는 것을 좋아하기 때문에ㅋㅋㅋ 최적의 데이터셋을 찾으려는 시도를 해보았다. 14만장의 이미지와 1.7만장의 이미지로 동일한 조건으로 학습시켰을 때 결과(코드는 모두 날아갔다... 미리 저장해뒀어야하는데ㅠ)
정확도나 F1은 그렇게 큰 차이가 나지 않지만 Loss값이 차이가 좀 나는 걸 볼 수 있다. 14만장 기준으로 train acc - val acc는 약 0.5%(0.9976 - 0.9923 = 0.0053) 정도로 정확도가 차이가 나고 1.7만장에서는 약 3.4%(0.9901 - 0.9563 = 0.0338 ) 차이가 난다. 14만장의 이미지를 학습한 경우가 더 다양한 이미지를 학습했고, 1.7만장만 학습한 모델은 제한된 개수 때문인지, 좀더 과적합됐다고 볼 수 있다. 
데이터셋의 크기가 클수록 Loss값은 감소하는 경향이 있지만, 어느 선에서 더이상 성능이 오르지 않는 선이 존재한다. 그래서 다양한 데이터셋 크기로 같은 조건에서 실험을 했다. 팀원들이 실험하고 있는 14만 장의 이미지, (데이터 어그멘테이션이 이미 강하게 들어가 있는 이미지들)를 기준으로 그 중에서 {10%, 25%, 50%, 75%, 100%}를 선택해서 학습에 활용하고, 성능 지표와 결과를 비교해보았다. 첫 에폭 때에는 차이가 좀 심했지만, 에폭이 6정도로 올라가니 비슷비슷해지는 모습을 보였다. (정확히는 폴드별로 미세한 차이가 있었으나 가장 잘나온 것 기준으로 함)
50% 데이터로 학습시켰을 때 첫 에폭에서 F1 스코어가 0.97, 75% 데이터로 학습시킬 때에는 F1가 0.99로...(?) 에폭 하나만 돌았는데 엄청나네; 이상하다.. 어쨌든 50%를 에폭 3번 돌면 75% 한번 도는 효과가 나는 정도로 빠르게 올랐다. 
음 여기까지 하다가.. 실험을 마무리하지 못하고 임의로 50%만 가지고도 에폭만 충분히 돌리면 좋은 성능을 낼 수 있겠다는 생각을 가지고 다른 실험으로 넘어갔다. (워낙 내가 만든 아웃풋 점수가 좋지 않아서.. 빨리 점수 올리고 싶은 조급함에 떠밀렸다ㅠ)
참고로 5개 폴드 모두 높은 성능이 나온다는 것은 모델이 데이터셋 전반에 걸쳐 안정적으로 학습된다는 의미 => 모델의 로버스트함을 주장할 수 있다. (특정 데이터에서만 우연히 좋은 성능이 나오는 것이 아니며, 여러 번의 검증을 통해 평균적인 성능을 파악할 수 있다.)


# NLP, Natural Language Processing 프로젝트

부트캠프에서 진행하는 3번째 경진대회 NLP가 시작됐다! 가장 기대하고 가장 관심이 많았던 주제였어서 뽜이팅이 넘친다. 일상 대회문을 요약하는 모델을 만드는 것이고, 이번에는 점수 평가 방법이 다른 대회들과 조금 다른 것 같다. 
인공지능 모델의 lifecycle: 학습 Training(대화문(문제)-요약문(정답)을 주고 모델이 학습하는 과정), 추론 Inference(학습된 모델을 사용하는 단계. 문제(입력)만 주고 정답을 예측/생성하게 하는 단계)

1. EDA
학습 데이터를 분석해보면, 대화문과 요약문은 약 5배 차이 정도 난다고. 나중에 인코더와 디코더의 최대 길이를 설정할 때 고려해야할 수도 있다. 그리고 주로 포멀한 대화와 비즈니스 관련 대화들이 있다. 은어나 약어는 나오지 않는다. 주요 대화 주제는 일상, 쇼핑, 통화, 인터뷰 등등이고, 개인정보가 들어간 부분은 모두 마스킹되어 있다.

2. 베이스라인 이해하기
- 전처리 
트레인 데이터셋을 데이터프레임으로 만든다. 데이터프레임에서 필요한 부분, "dialogue, summary"를 뽑아온다. 훈련할 때에는 인코더에 원본 대화 텍스트를 입력하고, 디코더에는 시작토큰과 실제 요약문을 입력한다. 그리고 디코더의 정답에는 실제 요약문과 종료 토큰을 사용한다.
테스트를 할 때에는 인코더로 원본 대화 텍스트를 입력하고 디코더에는 시작 토큰만 넣어줘서 모델이 직접 요약문을 만들게 한다. 
인코더와 디코더에 들어갈 데이터셋을 처리할 때:
input_ids (인코더)
attention_mask (인코더)
decoder_input_ids (디코더)
decoder_attention_mask (디코더)
labels (정답)
이렇게 5가지 키를 가진 하나의 딕셔너리로 만들어준다. 여기서 input_ids는 텍스트 원문을 토크나이저가 숫자로 변환한 시퀀스. 
attention_mask는 각 위치의 토큰이 실제 의미 있는 토큰인지(1) 패딩 토큰인지(0)를 나타내는 이진 마스크.
- 토큰화
텍스트 데이터 준비 -> 토큰화 -> 숫자(ID) -> 토큰화된 데이터로 구조화(위에서 정의한 Dataset 클래스로 모델이 요구하는 형식으로 정리해줌) 
원본 데이터가 텍스트로 있으면 -> prepare_train_dataset 함수안에 있는 tokenized_encoder_inputs로 대화문을 토큰화, 요약문도 토큰화 -> 데이터셋 클래스에서 인코더와 디코더에 넣을 입력 구성 -> 모델에 입력
최종적으로 모델에 들어가는 형태
# 인코더 입력: 숫자 ID의 시퀀스
# 디코더 입력: 숫자 ID의 시퀀스
# 둘 다 텐서(tensor) 형태로 변환되어 입력됨
- 학습, 평가지표 만들기
모델이 숫자를 출력(모델이 생성한 pred) -> 숫자를 텍스트로 변환 -> 특수 토큰(불필요한 토큰) 제거 -> 최종 텍스트
허깅페이스에서 Trainer 클래스가 자동으로 제공하는 기능으로 compute_metrics 함수 만듦.
pred 객체에는 pred.predictions(모델의 예측값), pred.label_ids(실제 정답 레이블) 두 가지 속성을 가진다. 
pred는 평가 단계에서 자동으로 생성되며 validation 데이터로 예측을 수행 -> 예측 결과와 실제 레이블을 pred 객체에 담아서 compute_metrics 함수 호출. 
숫자로 된 토큰은 다시 텍스트로 변환하고, 특수 토큰은 제거, ROUGE 점수 계산
- 추론 : 학습 중에 생성한 체크포인트 불러와서 추론

3. 시도해본 것들
- T5 모델을 찾아서 파인튜닝: 내가 찾은 모델은 한국어 요약을 프리트레인한 모델인데, 대화문에는 적합하지 않은건지ㅠ.. 결과에 이상한 말이 많이 써있다. 
- EXAONE 모델을 대화문 요약으로 파인튜닝한 모델이 있어서, 이 모델로 인퍼런스. 먼저 모델을 불러와준다. (트랜스포머 버전 안맞으면 로드가 안됨) 여기서 또 모델이 커서 그런건지 첨에 로드가 잘 안되서 load_in_8bit=True 라는 인자를 줬다. 이거 주려면 아래 인스톨 해야함.. 이게 뭐 모델을 손쉽게 압축할 수 있다나? 그래서 json에서 내용만 뽑아서 리턴하는 함수를 먼저 만들어주고, 그걸로 chat_list를 만들어준다. chat_list에 잘 저장이 된 걸 확인하고, 이걸 메세지에 프롬프트와 함께 넣어준다. 메세지에 잘 담겼다. 이제 이 메세지를 모델이 이해할 수 있도록 형식을 변환한다. PyTorch 텐서 형태로 반환한다. 메세지가 들어간 뒤에 출력을 얻는다. 
- 우리 팀에 챗지피티 4o mini를 파인튜닝시키신 분이 있는데(유료였고 약 3만원? 들었다고 한다) 그게 모든 점수들을 압도한다. 요약문을 잘 만들도록 학습된 모델을 가져와서 파인튜닝 시키는 방법들은 48점을 못넘기고 있었는데, 챗지피티 시키니까 52 찍었다!  
- 대화의 턴수와 주요 발화자를 토큰으로 추가해보자. 
가설 1, 대화의 턴수 정보와 발화자의 빈도를 세어서 턴에서 차지하는 비율을 토큰 정보로 주면 성능이 개선되지 않을까? --> 결과적으로 아니였던 걸로.. 
대화 턴수 토큰의 효과:
요약문의 길이와 상관관계가 있을 수 있음 (긴 대화는 더 긴 요약이 필요할 수 있음)
모델이 대화의 전체 구조를 파악하는데 도움
특히 encoder_max_len으로 잘린 대화의 경우, 원래 대화가 얼마나 길었는지 인식 가능
발화자 빈도 토큰의 효과:
주요 화자 식별을 통해 중요한 정보의 우선순위 결정 가능
대화의 주도자가 누구인지 파악하여 요약의 관점 설정에 도움
특히 한국어 대화는 주어 생략이 많은데, 주요 화자 정보가 이를 보완할 수 있음
데이터 전처리에서 대화 턴수와 주요 발화자 정보를 토큰으로 추가해서 넣어봤다. 추가된 토큰들을 살펴보니 딱히.. 메인스피커라던가 비율은 중요하지 않은 것 같다. 
대화가 몇 번 턴을 돌았느냐는 유의미한 정보같은데, 발화자 정보나 비중은 딱히 쓸모가 없는 것 같았음. person1, person2가 번갈아가면서 대화하는 형식이라서 비율이 거의 같아서..
그래서 턴 정보만 더하고, \n 으로 나뉜 것을 [TURN_SEP] 이라는 토큰으로 교체. 메모리 이슈로 계속 인코더 길이 맥스값을 원래대로 줄이고, 배치도 16으로 줄인거 치고는 나름(?) 괜찮은 결과가 나온 것 같다. 


# IR, information retrieval 프로젝트
월요일에 시작해서 목요일까지 우다다다 달려왔던 IR 대회. 짧고 굵게 끝나고 짧은 시간 탓에 블로그에 제대로 정리할 시간이 없었다. 어떤 시도들을 했고, 어떤 아이디어를 가지고 진행했는지 흐름을 되짚어보려고 한다. 
1. 대회 소개
- "질문"이 들어오면, 질문과 연관된 "적절한 문서"를 찾고, 그 문서를 참조해서 적절한 답변을 생성한다.
- 대회에서는 답변을 확인하지 않고, 답변을 위해 참조한 문서 3개(top k)를 뽑아서, 이 3개 문서가 잘 추출됐는지로 평가한다. 
- 임베딩 생성 모델, 검색 엔진, LLM을 활용할 수 있다. 
- 학습 데이터로 주어지는 문서와 쿼리 모두 jsonl 형태. 
2. 베이스라인 익히기 
- 베이스라인을 보면 한국어 임베딩 모델로 "snunlp/KR-SBERT-V40K-klueNLI-augSTS"를 사용하고, 엘라스틱서치에 문서들을 인덱스를 생성한 뒤에 sparse_retrieve, dense_retrieve를 이용해서 검색한다.
- 검색이 이뤄지는 과정: eval 파일의 라인을 한 줄씩 읽어서 llm에 보낸다. llm한테 처음에 보낼 때에는 persona_function_calling을 써서 과학과 관련된 질문인지 아닌지를 먼저 판단하게 하고, 과학과 관련된 질문인 경우에는 tools에 지정한대로 검색을 위한 쿼리를 작성한다. standalone_query에 작성해서 응답값에 넣어주고, 이 standalone_query를 sparse / dense 함수에 넣어 관련 문서를 검색해준다. 검색결과로 문서를 뽑은 뒤에 이것과 함께 다시 persona_qa로 llm에게 질문을 던진다. (주어진 레퍼런스를 활용해서 답변을 생성하라고). 만약 검색이 필요하지 않은 경우라면 그냥 대답해준다. 답변한 것들을 모아 output으로 저장해준다. 
정리하면, 쿼리 -> LLM 보내 -> 키워드 뽑아(standalone_query) -> 키워드로 엘라스틱서치에서 문서 검색 -> 문서 내용 가져와서 LLM에 참조 -> 답변 생성 
3. 초기 아이디어
- 베이스라인을 파악한 뒤에 시도해본 것은 먼저 프롬프트를 바꿔보는 것이였다. 가장 빠르게 시도해볼 수 있는 것이였고. 
persona_function_calling을 과학 관련 질문과 아닌 것을 좀 더 명확하게, 잘 구분할 수 있도록 바꾸어 보았다. 그리고 tools에서도 standalone_query를 작성하는 법에 대해서 좀 더 구체적으로 만들어 보았다. tools는 영어로 되어 있었는데 한글로 바꿔보기도 했다.
자잘자잘하게 올렸던 기록들. 처음에 프롬프트를 바꿨던 게 유의미한 성능 향상이 있었다. 
- 키워드로 엘라스틱서치에서 문서 검색 부분 바꾸기 :
베이스라인에서는 sparse_retrieve만 사용해서 검색 결과를 추출하고 있었다. 그래서 이거 대신에 dense_retrieve를 사용해봤더니 결과가 더 낮았다. dense 리트리브는 벡터 유사도를 이용한 검색 방법이고 sparse는 단어의 출현 여부를 고려해서 검색하는 방법이다. 두 개를 합치는 방식을 쓰면 어떨까 싶어서 hybrid retrieve를 만들었다.
hybrid retrieve는 먼저 sparse로 후보군을 추출하고(sparse가 성능이 더 좋게 나왔으니까 이걸로 일단 후보가 될 수 있는 문서 10개를 뽑아오면 더 유사한 문서들이 10개가 뽑힐 거라고 생각했다.) 그 다음에 dense로 추출된 문서들의 dense 유사도를 계산해서 sparse 점수 0.4와 dense 점수 0.6을 넣어 총 점수를 매겼다. 그리고 점수 순서로 재정렬해서 상위 3개 문서만 선택하는 방법이다. 이렇게 hybrid를 써보니까 0.45라는 애매한 점수가 나왔는데, 베이스라인인 sparse에서 얻은 0.42와 dense 0.37 보다는 어쨌든 높았다. 그래서 hybrid를 쓰는 건 유지하되, dense의 성능이 높아지면 hybrid 성능도 더 높아질 수 있겠다고 생각했다. 
----> dnese 성능을 높여보자 = 어떻게? 임베딩 모델을 더 좋은 걸 쓰면 문서와 쿼리 사이의 유사도를 더 정확하게 파악할 수 있다. 임베딩 모델을 바꿔서 dense 성능을 근본적으로 높여보자!
* 여기서 더 해보면 좋았을 것 : sparse와 dense의 비율을 좀 더 실험적으로 최적화해볼껄. 처음에 뽑는 문서양도 10개가 아닌 늘리거나 줄여볼껄. dense 검색에서 유사도를 계산할 때 다른 방식을 써봤어도 좋았을 것 같다.
4. 임베딩 모델 바꾸기
임베딩 모델은 텍스트를 고정된 크기의 숫자 벡터로 변환해주는 모델이다. 여러 가지 임베딩 모델을 사용해봤는데, 일단 베이스라인에 주어졌던 snunlp/KR-SBERT-V40K-klueNLI-augSTS부터 시작해서, 5개 정도 써본 것 같다. 
BM-K/KoSimCSE-bert -> 0.56
OpenAI text-embedding-3-small -> 0.64
Solar embedding -> 0.8 (다른 분은 같은 솔라 임베딩 써서 0.89 찍음) 
klue/bert-base -> 0.39,
BAAI/bge-large-en-v1.5 -> 0.37
5. 쿼리 3개씩 날려보기
gpt를 사용해서 원본 질의를 3개의 비슷한 질문으로 생성하게 했다. (동의어/유사어를 활용한 변형, 상위개념/하위개념을 포함한 변형, 인과관계 키워드를 추가한 변형) 이렇게 3개의 쿼리를 추가로 만들어서 하이브리드 리트리버에 추가한다. 하이브리드 리트리브는 각 질의에 따라 10개씩 문서를 뽑고, 그렇게 뽑힌 문서들 중에서 점수가 가장 높은 상위 3개 문서를 추출하는 방식이다. 
점수가 이렇게 해서 조금씩이나마 쭉쭉 오를 때 기분이 참 좋았다... 
오픈ai, solar가 성능이 좋았고(solar가 압도적으로 좋았음..) 나머지는 문서 뽑는 게 시원찮았다. 
처음에 solar 임베딩이 엘라스틱서치에 안들어가서(솔라 임베딩은 차원수가 4096이고, 엘라스틱서치에는 2048 이하만 들어간다고) 차원축소나 반으로 잘라서 사용하면 손실되는 정보가 있어서 제대로 결과가 안나올 것 같아 패스했다. 그런데 멘토링 시간에 멘토님이 솔라 임베딩을 강력 추천(?) 해주시길래 차원축소를 시도해서 넣어봤는데 아무리 해도 안되서.. 결국 앞에 2048을 넣는 걸로 잘라서 사용.. 그랬더니 결과가 또 이상하게 나오는거다?! 결국 솔라 임베딩에 성공한 팀원에서 임베딩한 문서와 쿼리 파일을 pkl 형태로 받아서 그걸로 사용했다. 
6. 시도는 했지만 결과가 좋지 않았던 방법들
- 오픈ai 임베딩으로 임배딩한 문서들을 10개 뽑고 그걸로 LLM에게 질문을 보고 가장 적절한 문서 3개를 뽑아달라고 한 것 -> 이거 점수가 생각보다 안좋게 나왔는데, 과학관련 질문인지 아닌지를 너무 빡세게 판별시켜서 그런 것 같다; 애초에 topk가 나온 문서가 많지 않았다... 이걸 좀 더 높일 수는 있었는데 LLM 쓰는 건 치트키같은 거고, AI 엔지니어링 관점에서 다른 시도들을 많이 해보는 게 좋겠다는 멘토님의 의견이 있어서 더이상 진행하지 않았다.
- sparse, dense 순서 바꾸는 방법:
내가 쓰던 hybrid 함수가 sparse로 먼저 10개 뽑고 dense가 들어가서 점수를 분배하는 방식이였는데, 이 방식에서 순서를 바꿔봤다. 
처음부터 dense로 문서를 10개 뽑고, sparse 검색으로 키워드 매칭이 좋은 문서 ID를 뽑는다. dense 검색으로 뽑힌 문서 중에서 sparse에서도 좋은 점수를 받은 문서만 남기고 나머지 제거, 필터링된 문서 중에서는 dense 스코어가 가장 높은 3개만 뽑는 방식이다. 그런데 sparse로 너무 많이 걸러져서 3개가 다 안뽑히는 경우들이 생겼다. 그래서 10개가 아니라 20개씩 뽑게 했고, sparse의 threshold도 0.5, 0.7 높여가면서 진행해봤다. 그런데 dense만 했을 때 0.4727, dense -> sparse로 해도 0.4727.. 성능도 거지같은데다가 심지어 dense만 한 것에 비해 아무런 변화가 없었다. 이건 dense에게 전폭적(?)인 지원을 한 방식인데 다시 하이브리드로 복귀하게된 계기가 됨.. 



# 부트캠프 지원부터 OT까지, 초반부 진행 후기

며칠 전에 살면서 단 한번도 꿔본 적 없는 꿈을 꿨다. 컴퓨터에서 VS code를 띄워놓고 무슨 코딩 문제를 끙끙거리면서 풀고 있던 꿈이였다. 솔직히 대학생때 전공 공부할 때에도 이렇게까지 공부를 열심히 하지 않았다. 꿈에 나올 정도로 요즘 몰두하고 있는 공부. 패스트캠퍼스에서 하고 있는 AI 부트캠프 4기 이야기이다. 
사실 인공지능은 꽤(?) 오래 전부터 관심이 많았고, 공부를 해보려고 깔짝거리기도 했었다. 사실 작년 9월에도 한번 크게 고민한 적이 있었다. 그때 지원을 할까말까 엄청 고민하고, 자기소개서를 쓰다가 결국은 지원을 안하고 다른 선택(?)을 했었다. 하지만 올해 5월에 다시 4기 공고를 봤을 때에는 지나칠 수 없었다. 한 일주일정도 진지하게 고민했던 것 같다. 왜냐하면 이번 길을 건너면 되돌아갈 수 없는 강을 건넌다는 느낌이 있었다. 컴퓨터 관련 전공자도 아니였고(비전공자)이면서 완전 다른 직군에서 커리어를 쌓고 있었던 상황에서 완전히 다른 직군으로 건너가고, 게다가 그걸 6개월 안에 부트캠프로 배운다? 솔직히 말하면 조금 무섭긴 했다. 
메일에 작년 9월에 자기소개서를 쓰다가 말았던 흔적이 남아있다. ㅋㅋㅋ 이번에는 6개월의 고민시간이 더 있었던 덕분인지, 한번 마음을 먹으니 지원 과정은 일사천리로 진행됐다. 
자기소개서는 그냥 항목 보고 바로 타이핑해서 넘어갔고, 글자수가 그렇게 길지 않았던 것 같다. 500자였나? 그리고 AI test는 처음에 하나하나 보면서 풀어보려고 했는데 문제 페이지가 넘어갈수록 용어 자체도 모르겠고 문제 자체를 이해하기도 어려웠다. 그냥 아무거나 찍을바에야 챗지피티한테 물어봐서 챗지피티 쓸 줄 안다는 거라도 보여줘야 낫지 않을까?(ㅋㅋㅋㅋㅋ합리화) 싶어서 도움을 많이 받았다. 이후에 비대면 녹화 면접은 자기소개서에서 나왔던 질문들을 바탕으로 인성(?) 면이나 지원 동기를 위주로 물어봤다. 비대면 녹화 면접은 2분인가? 한번 연습겸 말하고, 한번 더 녹화를 할 수 있었다. 
그리고 마음 편하게 결과를 기다리고 있던 중. 갑자기 불안한 마음이 들었다. 솔직히 부트캠프 경쟁률도 모르고, 몇명을 뽑고 몇명이 지원하는건지도 모르는데. 게다가 AI test에서 거의 다 챗지피티로 컨닝해서 냈는데 부도덕하다고 떨어지면 어떡하지? 자기소개서랑 비대면 녹화 면접을 엄청 대충 생각나는대로 바로 써서 냈는데. 너무 대충해서 떨어지면 어떡하지? 이런 걱정이 생겼다. 
혹시 떨어질 수도 있다는 생각에 다른 부트캠프도 찾아봤는데, 그때 찾아봤던 게 AI 키워드 가지고 나온 부트캠프들은 5개 정도 있었다. 
솔직히 뭐가 좋은지 세세하게 구분할 수 있는 능력은 없었고, 비트캠프라는 곳이 오프라인으로 하는 거라서 더 끌리기는 했다. 그런데 패스트캠퍼스는 AI업계에 대해 잘 모르는 나도 아는 '업스테이지'에서 콜라보(?)한 곳이니 온라인이긴 해도 퀄리티는 보장될 것이라는 생각은 지원하기 전부터 있었다. 패스트캠퍼스 떨어질수도 있으니 남은 곳들도 얼렁 지원서를 써봐야겠다... 하고 있었는데!! 
패스트캠퍼스에서 지원 결과를 예상보다 훨씬 빠르게 안내해주셨다. 최종 합격 메일을 보고 기쁘고 설레는 마음이 들면서도 내심 다른 곳에 더 지원안해도 되니 잘됐다는 안도감도 들었다. ㅋㅋㅋㅋㅋㅋ
합격 이후에 HRD 수강신청하고 슬랙에도 들어갔다. 이전 회사에서 슬랙을 써봤어서 어렵지는 않았다. 핸드폰 카톡으로 모두싸인이 와서 서명 몇 개 연달아 하고. 18일에 합격안내가 왔는데, 24일에 운영진 네트워킹 데이가 있었다. 
줌으로 다른 수강생들을 만나고 서로 인사하는 자리였는데, 줌이 익숙하지 않아서 기분이 진짜 이상했다. 그래도 4명씩 조를 편성해 나눠져서 돌아가면서 자기소개하고 서로 이것저것 질문도 하다보니 금방 적응됐다. 아 그리고 OT 전에 매니저님이랑 1:1 상담도 한번 했다. 꼼꼼히 케어해주시는 분위기라 온라인이여도 느슨하지가 않구나 싶었다. 
그리고 네트워킹 데이 이후부터 슬랙에 데일리공지로 매니저님께서 매일매일 공부할 것들을 올려주셨다. 과정을 본격적으로 공부하기 전에 미리 공부해두면 좋은 것들을 url로 정리해서 올려주셨는데 이게 진짜 큰 도움이 됐다. (양이 생각보다 많아서 전부 다 듣지는 못했다.) 그리고 정규 커리큘럼 전에 들으라고 패스트캠퍼스에서 강의도 하나 제공됐는데 이게 진짜 컸다. 
강의 이름이 데이터분석 레벨원 : 난생처음 배우는 파이썬 데이터분석
인데, 내가 들었던 파이썬 강의들 중에서 가장 깔끔하고 핵심이 쏙쏙 들어가있어서 미리 듣길 정말 정말 잘했다. (안그랬음 정규 과정에서 파이썬 못따라갔다 진심) 그리고 이 강의를 맡은 김용담 강사님은 우리 파이썬 강의로 줌 수업도 쭉 해주셔서 뭔가 줌수업에서 다시 보니 반가운 느낌도 ㅋㅋㅋㅋㅋ
두근두근 OT, 그리고 진짜 시작!
OT는 7월 16일 화요일이였다. 그런데 하필 이 날짜가 실업급여를 신청해서 무조건 가야한다는 1차 집합교육(장기수급자라서 무조건이라고) 날짜와 겹쳤다. OT 첫 날인데 놓치고 싶지 않아서 이어폰을 끼고 줌으로 들어가 들었다. 그래도 집합교육 와중에 듣기는 좀 어려웠고 12시부터 1시까지 점심시간이 있어서 그때 집에 와서 오후 시간에 본격적으로 참여했다. 
가장 궁금했던 게 앞으로 수업이 어떻게 진행될 것인지였는데 6개월치 타임테이블과 온라인강의로 들어야할 목록(?)이 담긴 스프레드시트를 받고 확인할 수 있었다. 앞으로의 난관이 펼쳐지는 순간. ^^...
그리고 각종 행정처리와 관련된 내용도 전달해주셨고, 신입이 회사 처음 들어갔을 때 한달 만근하면 연차 하나 생기는 시스템으로 한달 뒤에 휴가가 하루 생긴다고 한다. 오~~~ 휴가 쓸 생각 일도 없었지만 없는 것보다 좋지! (이전 기수까지는 한달에 하나 쌓이고 그달에 안쓰면 없어지는 것이였는데 이번에 규정이 바뀌어서 연달아 붙여서 쓸 수도 있다고 한다.) 
OT때 패리포터에 대한 안내도 있었는데, 이때 무조건 패리포터 지원해야겠다고 생각했다. 어차피 공부하면서 기록남기는 걸 좋아하니, 적성에도 맞고, 아직 과거 커리어를 잊지 않게 되는 효과도 있고. 
주저리주저리 글이 넘 길었다. 결과적으로 현재 부트캠프 4기에서 열심히 달리고 있는 지금, 부트캠프를 처음 지원해서 OT를 하기까지 극초반에 있었던 일들을 생각나는만큼 기록해보았다. 어쩌면 이 글을 보고 있을 누군가도, 몇 주 전의 나랑 똑같은 고민을 하고 있을지도 모른다. 그런 분들에게 조금의 힌트나 도움이 되길 바라면서 최대한 자세하게 기억나는대로 써봤다.
전공자가 4년 동안 배우고 각종 스터디와 대외활동으로 쌓을 능력치를 6개월이라는 짧은 시간에 압축해서 소화한다는 게. 솔직히 말하면 그게 가능한가? 싶기도 했다. 그런데, 지금은 생각이 조금 바뀌었다. 물론 모두 소화하기는 어려울지 몰라도, 자신이 욕심내는 만큼은 따라갈 수 있다는 것! 부디 이 마음을 끝까지 유지하길 바라며(나야 잘 들어..  미래의 나 보고있니?)
앞으로도 부트캠프가 진행되면서 중간중간 패리포터용으로 자세하게 후기를 남길 예정이다. 



