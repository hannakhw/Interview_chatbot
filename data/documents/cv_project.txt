Computer Vision 경진대회 기록

# 지금까지 했던 것
어그멘테이션: Torchvision이용해서 학습이미지 5배로 증강(뒤집기, 15도 회전, 블러 등 효과 랜덤하게 ->  Albumentation이용해서 학습이미지 10배로 증강 -> 더욱 강한 어그멘테이션 적용하여 17만장으로 증강 -> mixup, cutout, cutmix 온라인 어그멘테이션 적용
Validation set: StratifiedKFold로 폴드 5개 구성 후 하나를 밸리데이션 셋으로 사용하는 방식
모델: 초반에는 Densenet121, efficientnetb0, resnet34 모델에 각종 하이퍼파라미터 바꿔가며 실험, 후반에는 swin_tiny, swin_base 실험
앙상블: 하드보팅 시도, 폴드별 모델의 가중치를 평균내 최종 모델을 구성.

# 사이클별 대략적인 실험 개요 및 하이퍼파라미터 

## 첫 사이클
데이터셋: 기본 데이터셋에 Torchvision이용해서 학습이미지 5배로 증강(뒤집기, 15도 회전, 블러 등 효과 랜덤하게 들어감) 
모델: Densenet121, efficientnetb0, resnet34 3가지 모델
이미지 사이즈 32, 128, 260
배치 32
학습률 0.001
에폭 10
결과: 퍼블릭 기준 0.3924 매우 낮게 나옴 -> 어그멘테이션이 문제라고 생각이 들어 두 번째 사이클에서 어그멘테이션 숫자 늘림

## 두 번째 사이클
밸리데이션 세트를 따로 나누는 작업 + StratifiedKFold + mlflow 붙임
데이터셋: 기본 데이터셋에 Albumentation이용해서 학습이미지 10배로 증강 + mixup, cutout, cutmix 온라인 어그멘테이션 적용
모델: efficientnetb0, resnet34
이미지 사이즈 128
배치 32
학습률 0.001
에폭 10
결과: 퍼블릭 기준 0.45~0.48 -> 클래스 임밸런스가 된 이미지를 고려하지 않고 10배씩 증강해서 그런건가 싶어서 클래스 임밸런스를 적용해서 다음 사이클에서는 총 17만장을 만들고자 했음.

## 세 번째 사이클:
데이터셋: 더욱 강한 어그멘테이션 적용하여 17만장으로 증강시킨 이미지, 클래스 밸런스 맞춤 
모델: efficientnetb0, resnet34
이미지 사이즈 128
배치 32
학습률 0.001
에폭 10
결과: 퍼블릭 기준 0.80~0.82

## 네 번째 사이클:
데이터셋: 이미지 7만장  
모델: swin_tiny 
이미지 사이즈 224
배치 8
학습률 0.001
에폭 10
결과: 퍼블릭 기준 0.82

## 다섯 번째 사이클:
0.8에서 넘어가지 못하자 하드 보팅 앙상블을 써보자고 생각
데이터셋: 이미지 7만장, 14만장
모델: swin_tiny, efficientnetb4, resnet50, efficientnetb0, resnet34
이미지 사이즈 224, 128
배치 8, 32 
학습률 0.001
에폭 10
결과: 모델 5개 결과 하드보팅 시켰더니 0.91로 성능 향상

## 여섯 번째 사이클:
트랜스포머 모델 계열로 성능을 높여서 소프트보팅을 시키면 좋을 것 같아서 swin_tiny에 집중
데이터셋: 이미지 7만장 + 온라인 어그멘테이션(mixup, cutout, cutmix)
모델: swin_tiny
이미지 사이즈 224
배치 8 -> 32
학습률 0.001 -> 0.0001
에폭 3 -> 10
결과: 배치, 에폭 작게 돌릴 때 0.82 -> 늘렸더니 0.92

## 현재:
swin_tiny에서 base로 모델을 키워보고, 스케줄러, 라벨 스무딩 추가 적용 
데이터셋: 이미지 7만장 + 온라인 어그멘테이션(mixup, cutout, cutmix)
모델: swin_base
이미지 사이즈 224
배치 24
학습률 0.00035
에폭 15
결과: 모델 돌리는 중

