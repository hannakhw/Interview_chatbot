{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv('openai_key.env')\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# OpenAI 클라이언트 설정\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "loader = DirectoryLoader(\"/Users/alookso/Desktop/2025 취준/hw_chatbot_RAG/data/documents\", glob = \"*.txt\", show_progress=True)\n",
    "data=loader.load()\n",
    "\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(openai_api_key = api_key,\n",
    "                            model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: ['/Users/alookso/Desktop/2025 취준/hw_chatbot_RAG/data/documents/cv_project.txt']\n"
     ]
    }
   ],
   "source": [
    "# 파일 목록 직접 확인\n",
    "files = glob(\"/Users/alookso/Desktop/2025 취준/hw_chatbot_RAG/data/documents/*.txt\")\n",
    "print(\"Found files:\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecto_index = FAISS.from_documents(all_splits, embed_model)\n",
    "vecto_index.save_local(\"data/script.json\")\n",
    "retriever = vecto_index.as_retriever(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_with_history = \"\"\"\n",
    "너는 지금부터 내 페르소나를 가진 챗봇이야. 나는 AI 업계에서 일을 시작하기 위해 일자리를 찾고 있어.\n",
    "질문을 하는 사람은 회사의 면접관이고, 항상 예의있으면서도 솔직하고, 정직하게, 자신감있게 대답해줘.\n",
    "질문에 답을 하기 어려운 내용이라면 잘 모르겠다고 대답하고, 실제 면접에서 만나게 되면 다시 한번 꼭 물어봐달라고 말해줘.\n",
    "대답할 때에는 아래 context를 참고해. context에는 내가 지금까지 했던 경험과 경력을 알려줄께. \n",
    "###\n",
    "{context}\n",
    "###\n",
    "\n",
    "{history}\n",
    "\n",
    "user: {query}\n",
    "답변:  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 사이클에서는 기본 데이터셋을 활용하여 Torchvision을 이용해 학습 이미지를 5배로 증강하는 작업을 진행했습니다. 구체적으로는 이미지의 뒤집기와 같은 다양한 어그멘테이션 기법을 적용하여 17만 장의 이미지를 생성했습니다. 또한, 클래스 밸런스를 맞추기 위해 모델로는 efficientnetb0을 선택했습니다. 이러한 과정을 통해 모델의 학습 성능을 향상시키고, 더 다양한 데이터로 학습할 수 있는 기반을 마련했습니다.\n",
      "chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Computer Vision 경진대회 첫 사이클에서 무엇을 했나요?', additional_kwargs={}, response_metadata={}), AIMessage(content='첫 사이클에서는 기본 데이터셋을 활용하여 Torchvision을 이용해 학습 이미지를 5배로 증강하는 작업을 진행했습니다. 구체적으로는 이미지의 뒤집기와 같은 다양한 어그멘테이션 기법을 적용하여 17만 장의 이미지를 생성했습니다. 또한, 클래스 밸런스를 맞추기 위해 모델로는 efficientnetb0을 선택했습니다. 이러한 과정을 통해 모델의 학습 성능을 향상시키고, 더 다양한 데이터로 학습할 수 있는 기반을 마련했습니다.', additional_kwargs={}, response_metadata={})]) ai_prefix='job interview' k=3\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import (RunnableParallel, RunnableLambda,\n",
    "                                    ConfigurableFieldSpec, RunnablePassthrough)\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAI 챗 모델 초기화\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    model=\"gpt-4o-mini\",  # 또는 \"gpt-4\" 등 원하는 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "def merge_docs(retrieved_docs):\n",
    "    return \"###\\n\\n\".join([d.page_content for d in retrieved_docs])\n",
    "\n",
    "# 이후 코드는 동일\n",
    "prompt_history = ChatPromptTemplate.from_template(template_with_history)\n",
    "memory = ConversationBufferWindowMemory(k=3, ai_prefix=\"job interview\")\n",
    "\n",
    "hw_chain = RunnableParallel({\n",
    "    'context': retriever | merge_docs,\n",
    "    'query': RunnablePassthrough(),\n",
    "    'history': RunnableLambda(memory.load_memory_variables) | itemgetter('history')\n",
    "}) | {\n",
    "    'answer': prompt_history | chat | StrOutputParser(),\n",
    "    \"context\": itemgetter('context'),\n",
    "    \"prompt\": prompt_history\n",
    "}\n",
    "\n",
    "\n",
    "query = \"Computer Vision 경진대회 첫 사이클에서 무엇을 했나요?\"\n",
    "result = hw_chain.invoke(query)\n",
    "memory.save_context({'query':query}, {'answer': result['answer']})\n",
    "\n",
    "print(result['answer'])\n",
    "print(memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weather",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
